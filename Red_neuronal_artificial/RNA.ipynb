{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales artificiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "En el ámbito de *machine learning* las redes neuronales artificiales (RNA) tienen mucha aplicabilidad, tanto en las ciencias físicas como en otros áreas. Algunos ejemplos del uso de RNA son:\n",
    "\n",
    "* Reconocimiento automático de imagenes\n",
    "* Procesamiento de lenguajes naturales (lenguajes humanos)\n",
    "* Vehículos autonomos\n",
    "* Análisis de datos (*data mining*)\n",
    "* Clasificación de datos\n",
    "\n",
    "Es importante señalar que los RNA no son los únicos algorítmos de *machine learning*, y para muchas circunstancias el uso de una RNA ni siquiera es la mejor forma de resolver el problema. En este proyecto veremos como funcionan con un ejemplo simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la figura vemos un diagrama de la RNA que vamos a construir. Tiene 2 capas de neuronas (normalmente no incluimos la capa de neuronas de entrada cuando contamos el número de capas). Las capas entre la capa de entrada y la capa de salida se llaman *capas ocultas* de la red. En el caso de la red en la figura hay solamente una capa oculta. En cada capa tenemos neuronas artificiales, y entre las capas hay conexiones ponderadas. Este significa que asociada a cada conexión hay un número que se llama el *peso* de la conexión.\n",
    "\n",
    "|![](rna.png)|\n",
    "|------------|\n",
    "| Esquema de la red neuronal artificial |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la figura abajo vemos una neurona artificial, con 3 entradas y una salida. Asociada a cada neurona hay una *función de activación*.\n",
    "\n",
    "|![](neuron.png)|\n",
    "|---------------|\n",
    "|Una neurona artificial|\n",
    "\n",
    "En resumen tenemos los siguientes componentes en una RNA:\n",
    "\n",
    "* Una capa de entrada $\\vec{x}$.\n",
    "* Varias capas ocultas.\n",
    "* Una capa de salida $\\vec{y}$.\n",
    "* Un conjunto de pesos entre cada capa $W$.\n",
    "* Una función de activación, $\\sigma$, asociada a cada neurona en las capas ocultas y de salida.\n",
    "\n",
    "Hemos definido variables matemáticas arriba para expresar la operación de la red matemáticamente. En la capa de entrada, las neuronas no tienen función de activación, simplemente dan los valores de entrada a la red. Representamos estos valores en un vector $\\vec{x}$. Si hay $N$ neuronas en la capa de entrada, el vector $\\vec{x}$ tiene $N$ componentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 1 \n",
    "\n",
    "Considerar una red con $4$ neuronas en la capa de entrada. ¿Cuántos elementos tendrá el vector $\\vec{x}$? Considerar un conjunto de entradas donde la primera neurona tiene el valor $1$, la segunda $0.5$, la tercera $0.8$ y la cuarta $1.2$. Escribir el vector $\\vec{x}$ en este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solución\n",
    "\n",
    "Si hay $4$ neuronas en la capa de entrada, significa que hay $4$ entradas a la red, y por lo tanto $4$ elementos en el vector $\\vec{x}$.\n",
    "\n",
    "Para los valores dados en la pregunta el vector será:\n",
    "\n",
    "$\\vec{x} = \\begin{pmatrix} 1 \\\\ 0.5 \\\\ 0.8 \\\\ 1.2 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 2\n",
    "\n",
    "Considerar los siguientes vectores:\n",
    "\n",
    "$\\vec{x} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad \\quad \\vec{y} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 0 \\end{pmatrix}$. \n",
    "\n",
    "Calcular $|\\vec{x}|$, $|\\vec{y}|$, $\\vec{x} \\cdot \\vec{y}$. ¿Qué significan estos valores, geométricamente?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solución\n",
    "\n",
    "Con las componentes de los vectores se puede calcular:\n",
    "\n",
    "$|\\vec{x}| = \\sqrt{x_1^2 + x_2^2 + x_3^2 + x_4^2} = \\sqrt{1^2 + 1^2 + 0^2 + 0^2} = \\sqrt{2}$\n",
    "\n",
    "$|\\vec{y}| = \\sqrt{y_1^2 + y_2^2 + y_3^2 + y_4^2} = \\sqrt{0^2 + 1^2 + 2^2 + 0^2} = \\sqrt{5}$\n",
    "\n",
    "Estos son las longitudes (*normas*, *modulos*) de los vectores.\n",
    "\n",
    "Con la componentes también se puede calcular el *producto punto* (*producto escalar*):\n",
    "\n",
    "$\\vec{x} \\cdot \\vec{y} = x_1y_1 + x_2y_2 + x_3y_3 + x_4y_4 = 1 \\cdot 0 + 1 \\cdot 1 + 0 \\cdot 2 + 0 \\cdot 0 = 1$\n",
    "\n",
    "Este da la proyección del vector $\\vec{x}$ en el vector $\\vec{y}$. En el caso de tener dos vectores ortogonales, $\\vec{x} \\cdot \\vec{y} = 0$. Si son paralelos $\\vec{x} \\cdot \\vec{y} = |x||y|$.\n",
    "\n",
    "Puede ser que la expresión equivalente $\\vec{x} \\cdot \\vec{y} = |x||y| \\cos \\theta$ (donde $\\theta$ es el ángulo entre los vectores) es más familiar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función de activación\n",
    "\n",
    "Cada neurona en la capa oculta de la red tiene $3$ entradas que vienen de las neuronas de la capa de entrada. Cada neurona en la capa oculta tiene que hacer 3 cosas:\n",
    "\n",
    "* Calcular la suma ponderada de las entradas.\n",
    "* Usar esta suma ponderada como entrada a la función de activación.\n",
    "* Mandar la salida de la función de activación a la neurona en la capa de salida.\n",
    "\n",
    "La función de activación que usaremos en la red se llama la función *sigmoide*:\n",
    "\n",
    "$$\\sigma = \\frac{1}{1+e^{-x}} = \\frac{e^x}{e^x + 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 3\n",
    "\n",
    "Definir una función en Python que calcule $\\sigma(x)$ (usando la función `exp` de NumPy). Graficar los valores de la función para $-3 \\leq x \\leq 3$.\n",
    "\n",
    "La función `arange` de NumPy podría ser útil. Funciona así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2,\n",
       "       1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. , 2.1, 2.2, 2.3, 2.4, 2.5,\n",
       "       2.6, 2.7, 2.8, 2.9, 3. , 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8,\n",
       "       3.9, 4. , 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 5. , 5.1,\n",
       "       5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, 6. , 6.1, 6.2, 6.3, 6.4,\n",
       "       6.5, 6.6, 6.7, 6.8, 6.9, 7. , 7.1, 7.2, 7.3, 7.4, 7.5, 7.6, 7.7,\n",
       "       7.8, 7.9, 8. , 8.1, 8.2, 8.3, 8.4, 8.5, 8.6, 8.7, 8.8, 8.9, 9. ,\n",
       "       9.1, 9.2, 9.3, 9.4, 9.5, 9.6, 9.7, 9.8, 9.9])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arange(0,10,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(x):\n",
    "    return( 1.0/(exp(-x) + 1.0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = arange(-3,3,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb54bcfc7b8>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(x,sigmoide(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La operación de la red\n",
    "\n",
    "Cada conexión entre las neuronas tiene un *peso* asociado. Podemos representar estos pesos en un vector, con el mismo número de elementos que conexiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 4\n",
    "\n",
    "1. ¿Cuántas conexiones hay entre cada neurona en la capa oculta y la capa de entrada en la red en la figura al principio del Notebook?\n",
    "\n",
    "2. ¿Cuántas conexiones hay entre la neurona en la capa de salida y las neuronas en la capa oculta?\n",
    "\n",
    "3. Si representamos con un vector $\\vec{W}^{(1)}$ los pesos asociados a las conexiones entre neurona $1$ de la capa oculta y las neuronas de la capa de entrada, ¿cuántos elementos debe tener?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solución\n",
    "\n",
    "1. Hay $3$ conexiones entre cada neurona de la capa oculta y la capa de entrada, es decir, cada neurona de la capa oculta tiene una conexión con cada una de las neuronas de entrada.\n",
    "2. Hay $4$ conexiones, es decir, la neurona de salida tiene una conexión con cada una de las neuronas de la capa oculta.\n",
    "3. Ya que hay $3$ conexiones ($3$ entradas para la neurona $1$ de la capa oculta), el vector $\\vec{W}^{(1)}$ debe tener $3$ elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La suma ponderada de entradas\n",
    "\n",
    "Podemos calcular la suma ponderada de las entradas a la neurona $1$ en la capa oculta por el uso de la matemática de vectores:\n",
    "\n",
    "$S^{(1)} = \\sum_i W^{(1)}_i x_i = \\vec{W}^{(1)} \\cdot \\vec{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 5\n",
    "\n",
    "Si $\\vec{x} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ y $W^{(1)} = \\begin{pmatrix} 0.2 \\\\ 0.3 \\\\ -0.1 \\end{pmatrix}$, calcular $S^{(1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solución\n",
    "\n",
    "$S^{(1)} = \\vec{W}^{(1)} \\cdot \\vec{x} = 0.2 \\cdot 1 + 0.3 \\cdot 0 + (-0.1) \\cdot 0 = 0.2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representando todas las sumas ponderadas con una matriz\n",
    "\n",
    "Para la segunda neurona en la capa oculta podemos hacer lo mismo:\n",
    "\n",
    "$S^{(2)} = \\sum_i W^{(2)}_i x_i = \\vec{W}^{(2)} \\cdot \\vec{x}$\n",
    "\n",
    "y así para nueronas $3$ y $4$. Pero, en vez de tener $4$ ecuaciones separadas, podemos expresar todas las ecuaciones en una sóla con el uso de una matriz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 6 \n",
    "\n",
    "Las sumas ponderadas para cada neurona son $S^{(1)}$, $S^{(2)}$, $S^{(3)}$ y $S^{(4)}$. Podemos considerar un vector $\\vec{S}$ con elementos $\\vec{S} = (S^{(1)}, S^{(2)}, S^{(3)}, S^{(4)})$. Define una matriz $W$ de pesos en términos de los vectores $\\vec{W}^{(i)}$ tal que se pueda escribir\n",
    "\n",
    "$\\vec{S} = W \\cdot \\vec{x}$\n",
    "\n",
    "donde ahora tenemos $W$ (una matriz) multiplicando el vector de entrada $\\vec{x}$. Si hay $N$ neuronas en la capa oculta, y $M$ neuronas en la capa de entrada, y cada neurona de entrada está conectada a cada neurona oculta, ¿cuáles son las dimensiones de la matriz $W$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solución\n",
    "\n",
    "Vamos a acordarnos sobre las reglas de multiplicación de un vector por una matriz. Supongamos que tenemos una matriz $A$ con $3 \\times 3$ elementos, y un vector $\\vec{b}$ con $3$ elementos:\n",
    "\n",
    "$A \\cdot \\vec{b} = \\begin{pmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{31} & A_{32} & A_{33} \\end{pmatrix} \\begin{pmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{pmatrix} = \\begin{pmatrix} A_{11}b_1 + A_{12}b_2 + A_{13}b_3 \\\\ A_{21}b_1 + A_{22}b_2 + A_{23}b_3 \\\\ A_{31}b_1 + A_{32}b_2 + A_{33}b_3 \\end{pmatrix}$\n",
    "\n",
    "Entonces podemos escribir la matriz $A$ como\n",
    "\n",
    "$A = \\begin{pmatrix} A_1 \\\\ A_2 \\\\ A_3 \\end{pmatrix}$\n",
    "\n",
    "donde $A_1$ es un vector (de fila) con las componentes $A_{11}$, $A_{12}$ y $A_{13}$. Las componentes de los otros vectores de fila $A_2$ y $A_3$ se puede obtener en una manera similar. El producto ahora se puede escribir como\n",
    "\n",
    "$A \\cdot \\vec{b} = \\begin{pmatrix} A_1 \\\\ A_2 \\\\ A_3 \\end{pmatrix} \\cdot \\vec{b} = \\begin{pmatrix} A_1 \\cdot \\vec{b} \\\\ A_2 \\cdot \\vec{b} \\\\ A_3 \\cdot \\vec{b} \\end{pmatrix}$\n",
    "\n",
    "Entonces, ya que $S^{(i)} = \\vec{W}^{(i)} \\cdot \\vec{x}$, donde $i$ es un índice que indica la neurona de la capa oculta que corresponde, podemos escribir\n",
    "\n",
    "$W = \\begin{pmatrix} W^{(1)} \\\\ W^{(2)} \\\\ W^{(3)} \\\\ W^{(4)} \\end{pmatrix}$\n",
    "\n",
    "donde escribimos los $W^{(i)}$ como vectores de fila. Así las sumas ponderadas para todas las neuronas de la capa oculta se puede escribir como\n",
    "\n",
    "$\\vec{S} = W \\cdot \\vec{x}$.\n",
    "\n",
    "En este caso tenemos $N=4$ neuronas en la capa oculta y $M=3$ neuronas en la capa de entrada (y una conexión entre cada neurona en la capa oculta y cada una de las neuronas de la capa de entrada) así que la matriz $W$ tiene $4 \\times 3$ elementos.\n",
    "\n",
    "Generalmente, entonces, tenemos $N \\times M$ elementos en la matriz $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salida de la función de activación\n",
    "\n",
    "El resultado de la operación $W \\cdot \\vec{x}$ es un vector de sumas ponderadas, para cada neurona en la capa oculta. Todas las neuronas en esta capa (y en la capa de salida) tienen la misma función de activación: $\\sigma(x)$. Entonces la *salida* $\\vec{h}$ de la capa oculta (es decir, el conjunto de salidas de las neuronas en esta capa) será la función $\\sigma(x)$ aplicada a cada suma ponderada:\n",
    "\n",
    "$\\vec{h} = \\sigma(\\vec{S}) = \\sigma(W \\cdot \\vec{x})$\n",
    "\n",
    "donde entendemos que $\\sigma(\\vec{S})$ significa la operaci\\'on de la funci\\'on $\\sigma(x)$ aplicada a cada elemento del vector $\\vec{S}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 7\n",
    "\n",
    "Suponemos que todos los pesos en las entradas a neurona $1$ en la capa oculta son igual a $0.1$. En la notación que hemos usado tenemos \n",
    "\n",
    "$\\vec{W}^{(1)} = \\begin{pmatrix} 0.1 \\\\ 0.1 \\\\ 0.1 \\end{pmatrix}$.\n",
    "\n",
    "En una manera similar, suponemos que los pesos en las entradas de neurona $2$ son todos igual a $0.2$, para neurona $3$ son todos igual a $0.3$ y para neurona $4$ son todos igual a $0.4$. Si las entradas a la red (es decir, los valores que salen de las neuronas de entrada) son \n",
    "\n",
    "$\\vec{x} = \\begin{pmatrix} 1.0 \\\\ 0.5 \\\\ -1.0 \\end{pmatrix}$\n",
    "\n",
    "calcular las salidas de las neuronas en la capa oculta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solución\n",
    "\n",
    "Las salidas de la capa oculta están dadas por\n",
    "\n",
    "$\\vec{h} = \\sigma(\\vec{S})$\n",
    "\n",
    "donde $\\vec{S} = W \\cdot \\vec{x}$. Para el caso dado en la pregunta tenemos\n",
    "\n",
    "$\\vec{S} = \\begin{pmatrix} 0.1 & 0.1 & 0.1 \\\\ 0.2 & 0.2 & 0.2 \\\\ 0.3 & 0.3 & 0.3 \\\\ 0.4 & 0.4 & 0.4 \\end{pmatrix} \\begin{pmatrix} 1.0 \\\\ 0.5 \\\\ -1.0 \\end{pmatrix}$\n",
    "\n",
    "Podemos calcular el resultado con Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = array([[0.1, 0.1, 0.1],[0.2, 0.2, 0.2],[0.3, 0.3, 0.3],[0.4, 0.4, 0.4]])\n",
    "x = array([1.0, 0.5, -1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar la función `dot` de NumPy que calcula el producto punto entre dos vectores o el producto matricial entre un par de matrices (una de las matrices puede ser una matriz de solamente una columna/fila, es decir, un vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05, 0.1 , 0.15, 0.2 ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot(W,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces la salida de la capa oculta está dada por:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5124974 , 0.52497919, 0.53742985, 0.549834  ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoide(dot(W,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 8 \n",
    "\n",
    "Para calcular la suma ponderada de entradas a la capa de **salida**, podemos aplicar el mismo proceso que hemos visto. En este caso, hay $4$ entradas (los elementos del vector $\\vec{h}$) a una sóla neurona.\n",
    "\n",
    "1. ¿Cuáles son las dimensiones de la matriz de pesos $W_2$ para la capa de salida? \n",
    "2. Escribir la ecuación (con $\\vec{h}$) que define la suma ponderada $S_2$ de entradas a la capa de salida (ocupando notación vectorial/matricial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solución\n",
    "\n",
    "1. Hay una neurona en la capa de salida, así que $N=1$, usando la notación de Tarea 6. Hay $4$ neuronas en la capa oculta, así que $M=4$. Por lo tanto la matriz tiene dimensiones $1 \\times 4$: un vector de fila.\n",
    "2. La ecuación es\n",
    "\n",
    "$S_2 = W_2 \\cdot \\vec{h}$\n",
    "\n",
    "Es importante notar que $S_2$ es un *número*, NO un vector, ya que hay solamente $1$ neurona en la última capa de la red. Matemáticamente este es consistente, ya que $W_2$ es un vector de fila de $4$ elementos y $\\vec{h}$ es un vector de columna de $4$ elementos. El producto punto de dos vectores es un número."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 9 \n",
    "\n",
    "La salida de la última neurona (la de la capa de salida) también se calcula por la operación de la función $\\sigma(x)$ en la suma ponderada de entradas. Esta salida es la salida final de la red, $y$. Escribir la ecuación vectorial/matricial para $y$ en t\\'erminos de $\\sigma$, $W_2$, $W$ y $\\vec{x}$. Esta ecuación define la operación de la red!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solución\n",
    "\n",
    "Tenemos\n",
    "\n",
    "$y = \\sigma(S_2)$\n",
    "\n",
    "Podemos escribir $S_2$ en términos de $W_2$ y $\\vec{h}$:\n",
    "\n",
    "$y = \\sigma(W_2 \\cdot \\vec{h})$\n",
    "\n",
    "Pero $\\vec{h}$ depende de $\\vec{S}$:\n",
    "\n",
    "$y = \\sigma(W_2 \\cdot \\sigma(\\vec{S}))$\n",
    "\n",
    "y, finalmente, $\\vec{S}$ depende de $W$ y $\\vec{x}$:\n",
    "\n",
    "$y = \\sigma(W_2 \\cdot \\sigma(W \\cdot \\vec{x}))$\n",
    "\n",
    "La operación de la red está dada por la ecuación arriba, que depende de los pesos de las conexiones y las entradas a la red. Normalmente consideramos la función de activación como algo fijo para la red, así que no decimos que la salida final $y$ \"depende de\" $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función de costo\n",
    "\n",
    "Aunque tenemos la salida de la red, hay que determinar si el resultado es \"bueno\" o \"malo\". En términos matemáticos, queremos determinar el error en la salida. La función que usamos para cuantificar el error se llama la *función de costo*. En términos generales, necesitamos una función que determina la \"distancia\" entre el resultado de la red y el resultado correcto.\n",
    "\n",
    "De hecho, hay muchas funciones de costo: es otro aspecto importante del diseño de la red cual usamos. En este caso vamos a usar el error de la *suma de valores cuadrados* que está usado muchisimo en la ciencia y la estadística:\n",
    "\n",
    "$\\Delta = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "donde $\\hat{y}_i$ son los valores correctos, $y_i$ son las salidas de la red, y $\\Delta$ es la \"distancia\" entre estos valores. Por su similitud con la expresión matemática para la distancia cuadrada en un espacio euclidiano, a veces esta función se llama la *distancia euclidiana*. \n",
    "\n",
    "La suma es sobre el número de valores que tenemos que comparar. Hay solamente una salida de nuestra red, as\\'i que hay solamente un valor, y podemos ignorar la suma.\n",
    "\n",
    "Aunque la ecuación arriba parece muy simple, de hecho es una función multivariable que depende de todos los pesos en la red, como veremos más tarde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de la red: *backpropagation*\n",
    "\n",
    "Ahora hemos llegado a la parte más complicada (y más importante): cómo entrenar la red.\n",
    "\n",
    "La idea central de una red neuronal artificial es que puede mejorar su propia rendimiento sin intervención del programador. Es decir, la mejora del algoritmo es parte del algoritmo.\n",
    "\n",
    "¿Cómo funciona?\n",
    "\n",
    "Podemos determinar cuantitativamente el rendimiento de la red por la función de costo. Buscamos una salida $y$ de la red que *minimiza* la función de costo.\n",
    "\n",
    "Pero, tenemos que ajustar algo para cambiar la salida de la red. ¿Qué ajustamos? ¡Los pesos! La idea está mostrada en la figura abajo.\n",
    "\n",
    "|![](minimize.png)|\n",
    "|-----------------|\n",
    "|Minimización de la función de costo|\n",
    "\n",
    "Matemáticamente tenemos una función $\\Delta$ que depende de muchas variables (los pesos en $W$ y $W_2$). Buscamos el valor mínimo de esta función. Este es una aplicación del **cálculo diferencial**.\n",
    "\n",
    "En nuestra red tenemos $16$ conexiones. Entonces, la función de costo depende de $16$ variables. Matematicamente hablando, el problema que nos enfrenta es encontrar el mínimo de una función en un espacio de $16$ dimensiones! No es fácil encontrar el mínimo analíticamente (encontrando los ceros de las derivadas de la función de costo).\n",
    "\n",
    "En una red más complicada tenemos miles y miles de conexiones... No hay forma de encontrar el mínimo analíticamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo para encontrar mínimos\n",
    "\n",
    "Entonces, necesitamos un algoritmo computacional para encontrar el mínimo (o al menos uno de los mínimos) de la función de costo. Este problema matemático es algo muy común en la ciencia de datos, y se llama el problema de *optimización*.\n",
    "\n",
    "Aquí vamos a usar un algorítmo que se llama *descenso por gradiente*. En la figura arriba podemos ver (de nuevo) una representación esquemática de la función de costo (en realidad es una función de todos los $16$ pesos, no solamente uno). El punto rojo corresponde al valor de la función con los pesos iniciales. Podemos reducir la función de costo si seguimos el gradiente de la función hacia abajo. Por eso el método se llama *descenso por gradiente*. Eventualmente podemos llegar al mínimo (local) mostrado con el punto verde.\n",
    "\n",
    "Es importante notar que este método solamente puede encontrar un mínimo *local*, es decir, un punto donde la derivada de la función es igual a cero (y su segunda derivada es positiva). Puede ser que hay otro punto con estas propiedades que corresponde a un valor *menor* de la función de costo (otro mínimo). Entonces si tenemos la mala suerte de comenzar el algoritmo cerca a un mínimo local que no corresponde al mínimo global (el punto mínimo de verdad de la función de costo) nunca encontraremos el mínimo verdadero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Interludio matemático: derivadas parciales)\n",
    "\n",
    "Vamos a derivar una función de muchas variables (la función de costo), así que tenemos que aplicar **derivadas parciales**. Considerar una función de dos variables $f = f(x,y)$. La derivada parcial con respecto a $x$ se calcula considerando $y$ como una constante. De manera similar, la derivada parcial con respecto a $y$ se calcula considerando $x$ como una constante.\n",
    "\n",
    "Para dar un ejemplo, considerar la función $f(x,y) = x^2y + xy^2$. La derivada parcial con respecto a $x$ es:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x} = 2xy + y^2$\n",
    "\n",
    "Podemos ver que la derivada se toma considerando $y$ como una constante. La derivada parcial con respecto a $y$ es:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial y} = x^2 + 2xy$\n",
    "\n",
    "De nuevo, $x$ se considera como una constante en esta derivada. Otro resultado matemático muy importante que necesitamos es la **regla de cadena**. Para derivadas parciales funciona en una manera muy similar como para derivadas normales (o *totales* para usar la jerga). Entonces, podemos considerar una función $f = f(x(w,z),y(w,z))$ donde los argumentos de la función $x,y$ son, ellos mismos, funciones de otras variables $w,z$. Como ejemplo, la derivada con respecto a $z$ es:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial z} = \\frac{\\partial f}{\\partial x} \\frac{\\partial x}{\\partial z} + \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial z}$\n",
    "\n",
    "Tenemos arriba la regla de cadena aplicada para las variables $x$ y $y$. Hay que sumar estos términos para obtener la derivada de $f$ con respecto a $z$. Si $f$ solamente depende de $x$, por ejemplo, no tenemos el segundo término arriba. Podemos verificar este resultado con un ejemplo:\n",
    "\n",
    "$f(x(w,z),y(w,z) = x^2y + xy^2 \\quad \\quad x(w,z) = 2w+z \\quad \\quad y(w,z) = w + 3z$\n",
    "\n",
    "Entonces, la función escrita en términos de $w,z$ es\n",
    "\n",
    "$\\begin{split} f(w,z) &= (2w+z)^2(w+3z) + (2w+z)(w+3z)^2 \\\\ &= (4w^2 + z^2 + 4wz)(w+3z) + (2w+z)(w^2 + 9z^2 + 6wz) \\\\ &= 6w^3 + 37wz^2 + 29w^2z + 12z^3 \\end{split}$\n",
    "\n",
    "La derivada con respecto a $z$ es:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial z} = 74wz + 29w^2 + 36z^2$\n",
    "\n",
    "Ahora vamos a obtener este mismo resultado aplicando la regla de cadena. Las derivadas de $f(x,y)$ con respecto a $x$ y $y$ son:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x} = 2xy + y^2 \\quad \\quad \\frac{\\partial f}{\\partial y} = x^2 + 2xy$\n",
    "\n",
    "Las derivadas de $x,y$ con respecto a $z$ son:\n",
    "\n",
    "$\\frac{\\partial x}{\\partial z} = 1 \\quad \\quad \\frac{\\partial y}{\\partial z} = 3$\n",
    "\n",
    "Entonces, usando la regla de cadena tenemos:\n",
    "\n",
    "$\\begin{split} \\frac{\\partial f}{\\partial z} &= (2xy + y^2)\\cdot 1 + (x^2 + 2xy) \\cdot 3 \\\\ &= 8xy + y^2 + 3x^2 \\\\ &= 8(2w+z)(w+3z) + (w+3z)^2 + 3(2w+z)^2 \\\\ &= 74wz + 29w^2 + 36z^2 \\end{split}$\n",
    "\n",
    "Vamos a usar la regla de cadena para calcular la derivada de la función de costo con respecto a los pesos de la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivada de la función de costo con respecto a los pesos $W_2$\n",
    "\n",
    "Tenemos que calcular la derivada de la función de costo con respecto a los pesos. Para nuestra red, ya que hay solamente una salida, la función de costo es simple:\n",
    "\n",
    "$\\Delta = (y - \\hat{y})^2$\n",
    "\n",
    "Hay que acordar que $y$ es una función de los pesos! $\\hat{y}$ es solamente un número (una salida conocida). Por lo tanto la derivada con respecto a los pesos entre la capa oculta y la capa de salida es\n",
    "\n",
    "$\\frac{\\partial \\Delta}{\\partial W_2} = \\frac{\\partial \\Delta}{\\partial y}\\frac{\\partial y}{\\partial W_2} = 2(y - \\hat{y})\\frac{\\partial y}{\\partial W_2}$\n",
    "\n",
    "ya que $y$ es una función de $W_2$. Ahora tenemos que derivar $y$ con respecto a $W_2$. Ya hemos visto que $y$ es\n",
    "\n",
    "$y = \\sigma(W_2 \\cdot \\vec{h}) = \\sigma(z)$\n",
    "\n",
    "donde $\\vec{h}$ NO depende de $W_2$ y estamos escribiendo $z = W_2 \\cdot \\vec{h}$ para simplificar las ecuaciones. Entonces aplicando la regla de cadena de nuevo:\n",
    "\n",
    "$\\frac{\\partial y}{\\partial W_2} = \\frac{d \\sigma}{dz} \\frac{\\partial z}{\\partial W_2}$\n",
    "\n",
    "Ya que $\\sigma(z)$ solamente depende de una variable, podemos escribir su derivada como una derivada normal (total), es decir, $d\\sigma/dz$ en vez de $\\partial \\sigma/\\partial z$.\n",
    "\n",
    "$$\\frac{d\\sigma}{dz} = \\frac{d}{dz} \\frac{1}{1-e^{-z}} = -\\frac{1}{(1-e^{-z})^2} \\cdot (e^{-z}) = \\frac{1}{1-e^{-z}}\\left( \\frac{-e^{-z}}{1-e^{-z}} \\right) = \\sigma\\left( \\frac{-e^{-z}}{1-e^{-z}} \\right)$$\n",
    "\n",
    "Podemos simplificar lo que tenemos en parentesis:\n",
    "\n",
    "$$\\frac{-e^{-z}}{1-e^{-z}} = \\frac{1-e^{-z}}{1-e^{-z}} - \\frac{1}{1-e^{-z}} = 1-\\sigma$$\n",
    "\n",
    "Por lo tanto, la derivada de la función de activación es:\n",
    "\n",
    "$$\\frac{d\\sigma}{dz} = \\sigma(1-\\sigma)$$\n",
    "\n",
    "La otra derivada que necesitamos es:\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial W_2} = \\frac{\\partial}{\\partial W_2} W_2 \\cdot \\vec{h}$$\n",
    "\n",
    "**Ojo!** ¿Qué tenemos aquí? $W_2$ es un **vector**. ¿Cómo se puede derivar un vector con respecto a un vector? Tenemos que considerar esta derivada como $4$ derivadas en un vector (de columna), donde cada elemento de este vector corresponde a una derivada con respecto a ese componente. Explicitamente, podemos escribir\n",
    "\n",
    "$$\\frac{\\partial}{\\partial W_2} = \\begin{pmatrix} \\frac{\\partial}{\\partial W_{2(1)}} \\\\ \\frac{\\partial}{\\partial W_{2(2)}} \\\\ \\frac{\\partial}{\\partial W_{2(3)}} \\\\ \\frac{\\partial}{\\partial W_{2(4)}} \\end{pmatrix}$$\n",
    "\n",
    "donde los $4$ componentes del vector $W_2$ son $W_2 = \\left( W_{2(1)}, W_{2(2)}, W_{2(3)}, W_{2(4)} \\right)$. Por lo tanto tenemos\n",
    "\n",
    "$$\\frac{\\partial}{\\partial W_2} W_2 = \\begin{pmatrix} \\frac{\\partial}{\\partial W_{2(1)}} \\\\ \\frac{\\partial}{\\partial W_{2(2)}} \\\\ \\frac{\\partial}{\\partial W_{2(3)}} \\\\ \\frac{\\partial}{\\partial W_{2(4)}} \\end{pmatrix} \\cdot \\left( W_{2(1)}, W_{2(2)}, W_{2(3)}, W_{2(4)} \\right)$$\n",
    "\n",
    "Pero... para calcular el producto punto tenemos que tener un vector **de fila** primero y el segundo factor es un vector **de columna**. ¿Qué hacemos aquí? De hecho, hay una operación matemática que aplica en esta situación, se llama el **producto exterior**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Interludio matemático: producto exterior)\n",
    "\n",
    "Se puede definir el producto exterior de dos vectores de $4$ elementos así:\n",
    "\n",
    "$$\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix} \\otimes (y_1, y_2, y_3, y_4) = \\begin{pmatrix} x_1y_1 & x_1y_2 & x_1y_3 & x_1y_4 \\\\ x_2y_1 & x_2y_2 & x_2y_3 & x_2y_4 \\\\ x_3y_1 & x_3y_2 & x_3y_3 & x_3y_4 \\\\ x_4y_1 & x_4y_2 & x_4y_3 & x_4y_4 \\end{pmatrix}$$\n",
    "\n",
    "La definición con más o menos elementos es muy similar. Entonces el producto exterior de dos vectores es una *matriz*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuando con la derivada...\n",
    "\n",
    "Usando el producto exterior podemos ver que tenemos que aplicar todas las derivada $\\partial/\\partial W_2$ al primer elemento del vector $W_2$, que es $W_{2(1)}$. Pero la única derivada no igual a cero es la derivada con respecto a ese mismo componente. Usando la notación del interludio, tenemos $x_1y_1 = 1$ y todos los otros elementos en esa fila igual a cero.\n",
    "\n",
    "Para la segunda fila tenemos $x_2y_2 = 1$ y todos los otros elementos igual a cero. El resultado del producto exterior (y de la aplicación de las derivadas) es\n",
    "\n",
    "$$\\frac{\\partial}{\\partial W_2} W_2 = \\begin{pmatrix} \\frac{\\partial}{\\partial W_{2(1)}} \\\\ \\frac{\\partial}{\\partial W_{2(2)}} \\\\ \\frac{\\partial}{\\partial W_{2(3)}} \\\\ \\frac{\\partial}{\\partial W_{2(4)}} \\end{pmatrix} \\cdot \\left( W_{2(1)}, W_{2(2)}, W_{2(3)}, W_{2(4)} \\right) = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix}$$\n",
    "\n",
    "Esta matriz es la matriz de identidad de $4 \\times 4$ elementos. Multiplicando cualquier vector por esta matriz resulta en el mismo vector. Entonces tenemos:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial W_2} = \\sigma(1-\\sigma) \\cdot \\vec{h}$$\n",
    "\n",
    "La derivada de la función de costo con respecto a $W_2$ queda:\n",
    "\n",
    "$$\\frac{\\partial \\Delta}{\\partial W_2} = 2(\\hat{y}-y)\\sigma(1-\\sigma) \\cdot \\vec{h}$$\n",
    "\n",
    "Es importante acordar que el argumento para $\\sigma$ en la expresión arriba es $W_2 \\cdot \\vec{h}$ que es un **número**. Así que el producto punto arriba corresponde a multiplicar el vector $\\vec{h}$ por un número. El vector $\\vec{h}$ tiene $4$ componentes, entonces la derivada aquí es un **vector** de $4$ componentes. Estos componentes corresponden a los valores de la derivada para cada peso en el vector $W_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivada de la función de costo con respecto a los pesos $W_1$\n",
    "\n",
    "Ahora viene la parte más difícil del cálculo: obtener la derivada de la función de costo con respecto a los pesos entre la capa de entrada y la capa oculta. Vamos a tener que aplicar la regla de cadena varias veces.\n",
    "\n",
    "La función de costo, de nuevo, es:\n",
    "\n",
    "$\\Delta = (y - \\hat{y})^2$\n",
    "\n",
    "La salida de la red $y$ depende de $W_2$ (como vimos antes) y también $W_1$. Ya vimos la expresión matemática de $y$ en términos de $W_1$:\n",
    "\n",
    "$y = \\sigma_2(W_2 \\cdot \\sigma_1(W_1 \\cdot \\vec{x}))$\n",
    "\n",
    "Aquí tenemos una función de una función, por eso el cálculo de la derivada es más complicado. Para distinguir las funciones de activación (ya que tienen distintos argumentos) ponemos un índice. La función $\\sigma_2$ depende de $W_2$ y la función $\\sigma_1$ depende de $W_1$. \n",
    "\n",
    "Primero escribimos la ecuación usando $\\alpha = W_2 \\cdot \\sigma_1(\\beta)$ y $\\beta = W_1 \\cdot \\vec{x}$ para simplificar un poco la expresión:\n",
    "\n",
    "$y = \\sigma_2(\\alpha)$\n",
    "\n",
    "Por lo tanto:\n",
    "\n",
    "$$\\begin{split} \\frac{\\partial \\Delta}{\\partial W_1} = 2(y-\\hat{y}) \\frac{\\partial y}{\\partial W_1} &= 2(y-\\hat{y}) \\frac{dy}{d\\alpha} \\frac{\\partial \\alpha}{\\partial W_1} \\\\ &= 2(y-\\hat{y}) \\sigma_2(1-\\sigma_2) \\frac{\\partial}{\\partial W_1} W_2 \\cdot \\sigma_1(\\beta) \\end{split}$$\n",
    "\n",
    "La única cosa que depende de $W_1$ en la última derivada es $\\beta$, así que aplicamos la regla de cadena de nuevo:\n",
    "\n",
    "$$\\begin{split} \\frac{\\partial}{\\partial W_1} W_2 \\cdot \\sigma_1(\\beta) &= W_2 \\cdot \\frac{d\\sigma_1}{d\\beta} \\frac{\\partial \\beta}{\\partial W_1} \\\\ &= W_2 \\cdot \\sigma_1(1-\\sigma_1) \\cdot \\vec{x} \\end{split}$$\n",
    "\n",
    "Finalmente tenemos la derivada de la función de costo con respecto a los pesos $W_1$:\n",
    "\n",
    "$$\\frac{\\partial \\Delta}{\\partial W_1} = 2(y-\\hat{y}) \\sigma_2(1-\\sigma_2) W_2 \\cdot \\sigma_1(1-\\sigma_1) \\cdot \\vec{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Las derivadas de la función de costo\n",
    "\n",
    "Ahora resumamos lo que hemos calculado. La derivada de la función de costo con respecto a los pesos $W_2$ (entre la capa oculta y la capa de salida) es:\n",
    "\n",
    "$$\\frac{\\partial \\Delta}{\\partial W_2} = 2(\\hat{y}-y)\\sigma_2(1-\\sigma_2) \\cdot \\vec{h}$$\n",
    "\n",
    "donde usamos la notación $\\sigma_2$ para indicar que el argumento a la función de activación es $W_2 \\cdot \\vec{h}$.\n",
    "\n",
    "La derivada de $\\Delta$ con respecto a $W_1$ (entre la capa de entrada y la capa oculta) es:\n",
    "\n",
    "$$\\frac{\\partial \\Delta}{\\partial W_1} = 2(y-\\hat{y}) \\sigma_2(1-\\sigma_2) W_2 \\cdot \\sigma_1(1-\\sigma_1) \\cdot \\vec{x}$$\n",
    "\n",
    "#### Derivación automática\n",
    "\n",
    "Se puede apreciar que el cálculo de la derivada de $\\Delta$ se complica bastante con más capas de neuronas! Afortunadamente los módulos `TensorFlow` y `PyTorch` pueden calcular las derivadas **automáticamente**.\n",
    "\n",
    "También vale la pena explicar la razón por llamar el algoritmo *backpropagation*. Se puede considerar que calculamos las derivadas de la función de costo \"hacia atrás\", comenzando con la última capa de la red, y siguiendo con cada capa hasta que llegamos a la primera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 0.5\n",
    "w2 = 2.0\n",
    "w3 = 1.0\n",
    "w4 = -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = 1.0\n",
    "ytongo = 0.8\n",
    "\n",
    "epsilon = -0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return( 1.0/(exp(-x) + 1.0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xA = y0*w1\n",
    "xB = y0*w2\n",
    "\n",
    "yA = sigma(xA)\n",
    "yB = sigma(xB)\n",
    "\n",
    "xC = yA*w3 + yB*w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc = sigma(sigma(y0*w1)*w3 + sigma(y0*w2)*w4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5453898923131175"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = yc - ytongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w1 = epsilon * (2.0*E*sigma(xC)*(1.0-sigma(xC))*sigma(xA)*(1.0-sigma(xA))*y0*w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w2 = epsilon * (2.0*E*sigma(xC)*(1.0-sigma(xC))*sigma(xB)*(1.0-sigma(xB))*y0*w4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w3 = epsilon * (2.0*E*sigma(xC)*(1.0-sigma(xC))*yA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w4 = epsilon * (2.0*E*sigma(xC)*(1.0-sigma(xC))*yB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = w1 + delta_w1\n",
    "w2 = w2 + delta_w2\n",
    "w3 = w3 + delta_w3\n",
    "w4 = w4 + delta_w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc = sigma(sigma(y0*w1)*w3 + sigma(y0*w2)*w4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5492112058097233"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook a completar..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
